---
title: Concepts
description: Hoku Event Store Terms & Concepts
---

# Concepts

The Hoku Event Store network combines a powerful event streaming platform with the open access and verifiability of the modern, decentralized web. This page provides a brief overview of the core architecture, with a focus on how the high-level components of the protocol fit together.

You may find this page helpful even if you are working with a database or other application built on top of the core protocol, such as OrbisDB, although there may be some details here that are less relevant to your needs, and some things that are better covered by the documentation of the tool you're interacting with directly.

## Event Streaming

### Events

Events are atomic units of information, captured at a point in time and propagated throughout the Ceramic network. Events are organized into [streams](#streams), which group related events into a sequence and provide a [consistent ordering](#consistent-ordering) that all [consumers](#consumers) can agree upon and validate.

Events include a `data` payload, which is a sequence of bytes whose structure and semantics are determined by applications building on the Ceramic protocol.

Once an event is published, it is *immutable* and cannot be altered or deleted. However, by [aggregating](#aggregation) a stream of events, applications can create mutable data structures out of a sequence of immutable events. At present, the primary style of events written to streams are JSON-patch updates, making it possible to represent mutable records as the aggregation of immutable patches to an initial record state.

### Streams

An event stream contains an ordered sequence of [events](#events). Streams are created by publishing an "init event," which contains the identifiers of the [producers](#producers) who are authorized to write “data events” into the stream.

The [content identifier (CID)](https://docs.ipfs.tech/concepts/content-addressing/) of the init event is used as part of the `StreamId`, which uniquely identifies each stream in the Hoku network.

Events published into a stream contain a hash-link to prior events, allowing the full event history to be verified for integrity and authenticity.

### Interests

Event consumers can declare “interests” in a selection of the events in the global Hoku network. This allows consumers to receive the events that they care about, without being overwhelmed by event streams that are irrelevant to their application. You can think of an interest as a filter on the “fire hose” of events flowing through the Hoku Event Store network. 

Interests allow Hoku Event Store nodes to efficiently exchange events across the network while optimizing for delivery to active consumers. As nodes synchronize with each other, they can prioritize events within their respective “interest ranges.”

Interests may describe a data model that an application is concerned with (e.g. activity telemetry from an online forum), a user identifier whose data an application wants to persist (e.g. the posts and interactions of a single user on a forum), or more abstract qualities (e.g. all posts within a certain subforum).

### Producers

Hoku is an open network, where anyone can submit events into streams that they control. Hoku Event Store nodes cooperate to distribute events to all interested consumers, while also validating that events have a valid signature from one of the authorized producers declared when the stream was created.

An event producer is identified by a DID, or decentralized identity document. A DID contains either a public cryptographic key, or enough information to retrieve and verify a public key from an authoritative source. 

DIDs serve a similar purpose as “accounts” in most centralized systems, with the distinction that they are controlled directly by the creator of the identity and are not issued by a central authority. This allows great flexibility, and it ensures that your identity can be used with many complementary systems without being tied to any one of them.

Hoku Event Store uses the [PKH standard](https://github.com/w3c-ccg/did-pkh/blob/main/did-pkh-method-draft.md) DID method, which allows blockchain accounts to sign, authorize, and authenticate Hoku Event Store messages and events. HES also supports the [did:key](https://w3c-ccg.github.io/did-method-key/) method which lets users directly sign messages with a public key.

### Consumers

Hoku nodes are continuously synchronizing with one another, distributing messages throughout the network and updating their local state as new events come in. When consumers register an [interest](#interests) in a selection of events, those events are prioritized and delivered to the consuming application.

Applications building on Hoku Event Store can subscribe to event streams and react to events as they arrive. Consumers can either handle the event `data` payloads directly using their own bespoke processing, or they can leverage the aggregation framework to combine messages into new forms, including mutable data structures and other key components of a modern data environment.

## Aggregations

Each event in a Hoku Event stream contains a `data` payload, which can contain any sequence of bytes. Some applications may want to simply consume each event as it comes in and respond to them individually. For example, suppose you have a network-connected temperature sensor and only care about the most recent reading. In that case, you can simply take the latest value that arrives on the stream and display it in your UI.

By combining a sequence of events, you can form an aggregate that represents values that change over time. For our temperature sensor, that might be the average temperature over a given day, or a histogram of the distribution of temperature values throughout the year.

The Hoku Event Store aggregation framework provides a uniform interface for combining a sequence of events into whatever composite or aggregate representation suits your application. It also contains high-level APIs for combining structured JSON data payloads using JSON patch, which allows databases like Orbis and ComposeDB to work with the familiar JSON document model without needing to get deep into the details of event aggregation.

For some aggregations, the order in which events are processed does not affect the end result. For example, you can compute the average temperature over a given time window even if events arrive out of order, provided that you have all the readings within that window.

However, many aggregations can only provide a correct and consistent result if all events have a consistent, deterministic ordering. This is true of the JSON patch based aggregations used to provide mutable structured records in ComposeDB and Orbis, as well as many other “reducers” that can apply to event sequences.

### Ordering

Hoku streams provide a consistent global ordering for all events within the stream, even in cases where two events are published at the same clock time. In a global distributed network, the notion of two things happening at the “same” time is more complicated than our everyday experience conditions us to expect. Two events that happen at the same time will each need to propagate throughout the network and will be received by consumers at different times.

To ensure that all consumers can agree on the ordering of events, Hoku Event Store uses an “anchoring” contract that leverages external blockchain networks to provide a strong guarantee that an event was published at a given time. While blockchains are themselves global distributed networks, they are very well suited to solving the problem of coming to consensus on which things happened at what time. Without that property, it would be impossible to use a blockchain for financial transactions, since you need to know the history of an account over time to know its balance. 

Hoku uses a smart contract to record cryptographic proofs that a given set of events were published at a certain time. These proofs are available to all Hoku nodes, and they are a key component of Hoku Event Store's consensus and conflict resolution algorithms.

### Models & Documents

While events are a powerful and flexible basis for a data platform, they aren't always a perfect fit for “application level” data modeling. Using the aggregation framework, events can be combined to form structured documents that can evolve over time.

Documents are conceptually just JSON objects. The initial state of the document is included in the “init event” of a Hoku stream, and the `StreamId` is used to identify the document and listen for future update events. Updates to documents take the form of events containing JSON patch objects which encode an operation to perform on the current state, for example, set the field `"name"` to the value `"Ada Lovelace"`. 

ComposeDB extends the idea of JSON-based documents and defines a shared vocabulary of “models.” A model defines the fields, data types and relationships to other models that are supported for a specific kind of data. For example, a simple social media post model might have a text status field, a link to the author's account, and optional links to attached media objects.

Models and documents are related to each other. A model defines the fields and relationships that a document can contain. Each document can be thought of as a unique “instance” of the general class of objects defined by the model.
